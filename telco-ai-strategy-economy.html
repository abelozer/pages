<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Telco AI Strategy and Economy</title>
  <meta name="description" content="Telco AI strategy and economy: positioning, trust model, monetization paths, and KPI framework.">
  <link rel="stylesheet" href="site.css">
</head>
<body class="full-report-page">
  <main class="wrap">
    <p class="back-nav">
      <a class="back-link" href="index.html">‚Üê Back to index</a>
    </p>

    <header class="hero">
      <p class="eyebrow">Telco AI Report</p>
      <h1>Telco's AI Strategy &amp; Economy</h1>
      <p class="lede">
        A complete guide to telco AI strategy: positioning, trust framework, monetization tracks, and the operational economics model for telecom operators.
      </p>
      <div class="hero-ribbon" aria-hidden="true">
        <span></span><span></span><span></span>
      </div>
      <figure class="section-figure">
        <img src="img/telco-edge-inference.svg" alt="Diagram showing distributed inference across edge POPs, metro data centers, and core data centers">
      </figure>
    </header>

    <section class="panel">
      <h2>Three Ways for Telcos</h2>
      <ul>
        <li><strong>AI Landlord:</strong> provide power, rack, and connectivity.</li>
        <li><strong>Sovereign AI Operator:</strong> vertical solutions for regulated markets.</li>
        <li><strong>AI-Integrated Network:</strong> focus on efficiency and productivity.</li>
      </ul>
    </section>

    <section class="panel">
      <h2>Telcos Positioning</h2>
      <p>
        Unlike hyperscalers who optimize for massive model training, telcos are positioned to win at efficient inference by leveraging distributed edge infrastructure. Often, when price and latency are equal, hyperscalers still win thanks to richer ecosystems.
      </p>
      <div class="columns">
        <article class="mini-panel">
          <h3>Advantages</h3>
          <ol>
            <li>Know local markets and have trusted enterprise relationships.</li>
            <li>Own last-mile and deep network facilities and can deploy edge compute.</li>
            <li>Can play on sovereignty and local-regulation compliance.</li>
            <li>May offer more deterministic performance than layered best-effort hyperscaler stacks.</li>
            <li>Can integrate AI into network control loops to maximize performance per watt per dollar in real time:
              <ul>
                <li>Improve performance.</li>
                <li>Lower operational cost.</li>
                <li>Increase energy efficiency.</li>
                <li>Enable new revenue models.</li>
              </ul>
            </li>
          </ol>
        </article>
        <article class="mini-panel">
          <h3>Disadvantages</h3>
          <ol>
            <li>Cannot out-build hyperscalers.</li>
            <li>Legacy technology stack and immature software ecosystem.</li>
            <li>Skill shortage (single-digit teams vs 10x-100x scale at hyperscalers).</li>
            <li>Significantly lower capital and scale.</li>
          </ol>
        </article>
      </div>
      <p class="callout callout-fact"><span class="callout-icon" aria-hidden="true">üéØ</span>Telcos cannot win at hyperscale training, but can win at distributed inference scale.</p>
    </section>

    <section class="panel">
      <h2>What Is TRUST for Telcos</h2>
      <p class="callout callout-warning"><span class="callout-icon" aria-hidden="true">‚ö†Ô∏è</span>Trust is not automatic. Trust in reliability is not equal to trust in digital agility. Trust needs products.</p>
      <figure class="section-figure">
        <img src="img/trust-pillars.svg" alt="Trust as a Service pillars including residency, auditability, isolation, routing, and energy transparency">
      </figure>
      <article class="mini-panel">
        <h3>1. Regulatory &amp; Sovereignty Trust</h3>
        <p>Telcos are regulated infrastructure operators under national telecom licenses, lawful intercept obligations, data retention laws, security audits, and government oversight.</p>
        <p><strong>What this means:</strong> data location guarantees, compliance with national law, auditable infrastructure, and legal accountability inside jurisdiction.</p>
      </article>
      <article class="mini-panel">
        <h3>2. Infrastructure Trust</h3>
        <p>Ownership of fiber, last-mile access, metro POPs, mobile RAN, core routing, and national backbones enables deterministic latency, physical redundancy, and SLAs backed by physical control.</p>
      </article>
      <article class="mini-panel">
        <h3>3. Operational Trust (SLA &amp; Reliability)</h3>
        <ul>
          <li>99.999% uptime expectations.</li>
          <li>Emergency services requirements.</li>
          <li>Disaster recovery mandates.</li>
          <li>Carrier-grade processes.</li>
        </ul>
      </article>
      <article class="mini-panel">
        <h3>4. Data Relationship Trust</h3>
        <ul>
          <li>Subscriber identities.</li>
          <li>Billing data.</li>
          <li>Location data.</li>
          <li>Network behavior metadata.</li>
        </ul>
      </article>
      <article class="mini-panel">
        <h3>5. National Strategic Role</h3>
        <ul>
          <li>Critical national infrastructure.</li>
          <li>Part of national resilience planning.</li>
          <li>Cybersecurity partners to government.</li>
        </ul>
        <p>That gives: political legitimacy, long-term stability perception, and institutional embeddedness.</p>
      </article>
      <article class="mini-panel">
        <h3>6. Model Governance Trust</h3>
        <ul>
          <li>Transparency about model usage.</li>
          <li>No silent data harvesting.</li>
          <li>Clear retention policies.</li>
          <li>Auditability of inference logs.</li>
        </ul>
      </article>
      <article class="mini-panel">
        <h3>7. Energy &amp; ESG Trust</h3>
        <ul>
          <li>Clean hydropower.</li>
          <li>Transparent energy sourcing.</li>
          <li>Carbon accounting.</li>
        </ul>
      </article>
      <article class="mini-panel">
        <h3>8. AI Deterministic Performance Trust</h3>
        <p>Hyperscalers and neoclouds often win in developer trust, AI innovation credibility, and ecosystem richness.</p>
        <p><strong>If trust is your asset, productize it:</strong> Trust-as-a-Service.</p>
        <ul>
          <li>Data residency enforcement.</li>
          <li>Audit-ready logging.</li>
          <li>Physically pinned compute zones.</li>
          <li>Transparent topology mapping.</li>
          <li>In-country control plane.</li>
          <li>AI-priority routing.</li>
          <li>Regulatory-grade documentation.</li>
          <li>Energy source transparency.</li>
          <li>Data non-retention guarantees.</li>
          <li>Model isolation per tenant.</li>
          <li>Tok/$ dashboards.</li>
          <li>GPU-hour billing clarity.</li>
        </ul>
      </article>
    </section>

    <section class="panel">
      <h2>Operations / AI for Productivity</h2>
      <p>
        AI for Network transforms telecom operations from manually managed infrastructure to a self-optimizing, energy-aware, economically intelligent system. The primary value is internal margin expansion, resilience, and SLA uplift, while enabling premium deterministic service models.
      </p>
      <figure class="section-figure">
        <img src="img/ai-network-loop.svg" alt="Observe predict act loop for AI-driven network operations">
      </figure>
      <ol>
        <li><strong>Predictive Maintenance and Fault Prediction:</strong> ML (for example, Random Forest and LSTM) over OTDR/OSA history predicts fiber cuts and hardware degradation before failure.</li>
        <li><strong>Energy Efficiency Optimization:</strong> dynamic power allocation and sleep modes reduce energy consumption by roughly 15% without harming user experience.</li>
        <li><strong>Automated &amp; Intelligent Customer Support:</strong> generative assistants use customer history and real-time network state (example: Vodafone).</li>
        <li><strong>Intelligent Capacity Planning.</strong></li>
        <li><strong>GAI for Network Engineering &amp; Assisted Troubleshooting</strong> (Cisco AI Canvas).</li>
        <li><strong>Proactive Service Monitoring &amp; Customer Churn Reduction.</strong></li>
        <li><strong>Reduction of Truck Rolls:</strong> remote root-cause diagnostics reduce physical site visits and costs.</li>
        <li><strong>Network Operations and Autonomous Optimization:</strong> move toward Level 4 autonomy with minimal human intervention.</li>
        <li><strong>RAN Visibility &amp; Optimization:</strong> deeper traffic visibility, better congestion experience, and better power control.</li>
      </ol>
    </section>

    <section class="panel">
      <h2>AI Infra / Hardware Resources</h2>
      <p>Telcos are capitalizing on physical assets (data centers and edge locations) and regulatory expertise to enter the compute market.</p>
      <ol>
        <li>GPUaaS.</li>
        <li>Compute/GPU Marketplace.</li>
        <li>Fine-tuning service.</li>
        <li>Hosting pre-trained LLMs in customer tenants.</li>
        <li>Dedicated AI transport (Golden Pipes: Verizon AI Connect, SK Telecom).</li>
        <li>MOFN &amp; managed IP backend networks.</li>
        <li>Hosting GPUs (rack, space, cooling, and related facilities).</li>
        <li>Connectivity Quality on Demand (QoD) and differentiated experience.</li>
        <li>Monetize AI connectivity through Network-as-a-Service (NaaS).</li>
      </ol>
      <p>GPUaaS is becoming a commodity. It is often cheaper to rent compute from AWS, Google Cloud, Lambda, and CoreWeave. This market can quickly become a race to the bottom.</p>
    </section>

    <section class="panel">
      <h2>AI Software Services / AI-Based Software Solutions</h2>
      <p>
        Sovereign AI infrastructure requirements and data privacy rules drive software opportunities. Most services target enterprises, with fewer globally recognized opportunities for residential subscribers.
      </p>
      <article class="mini-panel">
        <h3>AI Services for Enterprises</h3>
        <ol>
          <li>Host sovereign generic AI features (model catalogues) in customer tenants (for example, Swiss AI Platform):
            <ul>
              <li>Text summarization.</li>
              <li>Translation.</li>
              <li>RAG frameworks.</li>
              <li>Agentic frameworks.</li>
              <li>Call recordings processing.</li>
              <li>Meeting minutes drafting.</li>
              <li>Text-to-image (ideation and creative use).</li>
              <li>Image-to-text (object recognition).</li>
              <li>Speech-to-text.</li>
              <li>Text-to-speech.</li>
            </ul>
          </li>
          <li>Data lakes and long-term storage (S3).</li>
          <li>LLM incremental fine-tuning.</li>
          <li>Cisco AI Defense for model validation.</li>
          <li>LLM intelligent routing (including down detection and price/quota-driven routing).</li>
          <li>ThousandEyes dashboard for popular AI services.</li>
          <li>Integrated Sensing and Communication (ISAC).</li>
          <li>Vertical-specific AI solutions (manufacturing, healthcare, smart cities, logistics, utilities, and others).</li>
        </ol>
      </article>
      <article class="mini-panel">
        <h3>AI Services for Residential Market</h3>
        <p class="callout callout-note"><span class="callout-icon" aria-hidden="true">‚ÑπÔ∏è</span>This is a nice-to-have portfolio and more of a retention tool than a growth engine.</p>
        <ol>
          <li><strong>Smart Home Management:</strong> AI in residential gateways optimizes Wi-Fi and QoE for high-bandwidth applications (for example, 4K streaming).</li>
          <li><strong>IoT and Home Security:</strong> AI-driven bundles monitor household traffic anomalies and protect vulnerable devices.</li>
          <li><strong>AI-based parental control:</strong> guardrails with tools such as Cisco AI Defense.</li>
          <li><strong>Hyper-personalization:</strong> usage-based plans, contextual troubleshooting, predictive care, and hardware upsell.</li>
        </ol>
        <p>A smart thermostat attempting to contact an unknown external IP is one example: an RNN can detect the anomaly in time-series data and the system can isolate the device to prevent botnet recruitment.</p>
        <p>Core value proposition: simple router-level security without per-device software installation.</p>
        <ul>
          <li>Protect headless devices that cannot run antivirus software.</li>
          <li>Proactively scan IoT endpoints for default passwords and outdated firmware.</li>
          <li>Analyze content usage in real time to enforce categories and screen-time limits.</li>
          <li>Analyze RF disruptions to detect motion and known/unknown devices.</li>
        </ul>
      </article>
    </section>

    <section class="panel">
      <h2>Sell Experience</h2>
      <ol>
        <li>Marketplace for ecosystem partners.</li>
        <li>Education, courses, and trainings.</li>
        <li>Data monetization.</li>
      </ol>
    </section>

    <section class="panel">
      <h2>Other Possible Moves</h2>
      <ol>
        <li><strong>Partner with neocloud providers:</strong> a primary strategy for entering AI without full factory CAPEX. Telcos can resell or host neocloud capacity, or lease facilities in an IaaS model.</li>
        <li><strong>AI Economy Enablers:</strong> avoid competing head-on with hyperscalers; build specialized infrastructure and connectivity needed to run AI.</li>
        <li><strong>DePINs:</strong> decentralized physical infrastructure with isolated remote sites and overlay links to major POPs (examples: SRv6 overlay with eBPF on Linux, isolated PE routers).</li>
        <li><strong>Contribute to decentralized GPU clouds</strong> to increase GPU utilization.</li>
      </ol>
    </section>

    <section class="panel">
      <h2>Tokens per Second per Watt</h2>
      <p>
        Tokens per second per watt (TPS/W) is a critical efficiency metric for AI, measuring how much generation performance can be achieved for a given power footprint (including hardware and cooling). As power becomes a hard constraint for AI expansion, TPS/W becomes a core data-center metric.
      </p>
      <p>
        To optimize this, attack the denominator (energy) while maintaining acceptable latency in the numerator. This is where edge inference and inference-optimized chips matter.
      </p>
      <p class="callout callout-warning"><span class="callout-icon" aria-hidden="true">‚ö†Ô∏è</span>Optimizing TPS/W without actual sell is marginal. TPS/W is mostly an operational metric: it lowers cost and increases margin.</p>
      <figure class="section-figure">
        <img src="img/tpsw-kpi-board.svg" alt="Operations board with utilization, batching, and model size efficiency controls">
      </figure>
      <article class="mini-panel">
        <h3>Power Consumers</h3>
        <ul>
          <li>GPU.</li>
          <li>CPU.</li>
          <li>DRAM.</li>
          <li>NVLink / PCIe.</li>
          <li>Networking.</li>
          <li>Storage.</li>
          <li>Cooling overhead.</li>
        </ul>
      </article>
      <article class="mini-panel">
        <h3>Where Telcos Lose Tok/s/W Today</h3>
        <ol>
          <li>Over-sized models for simple use cases.</li>
          <li>Poor batching.</li>
          <li>Underutilized GPUs.</li>
          <li>No quantization.</li>
          <li>Air-cooled legacy data centers.</li>
          <li>Single-tenant deployments.</li>
        </ol>
      </article>
      <p>Right GPU choice significantly improves TPS/W.</p>
    </section>

    <section class="panel">
      <h2>List of KPIs to Measure</h2>
      <ul class="kpi-list">
        <li><strong>Tok/s/W</strong></li>
        <li><strong>Tok/$</strong></li>
        <li><strong>Tok/CO<sub>2</sub></strong></li>
        <li><strong>Revenue per GPU-hour</strong></li>
        <li><strong>Utilization %</strong></li>
      </ul>
    </section>

    <section class="panel">
      <h2>Attacking Energy in TPS/W Equation</h2>
      <ol>
        <li>Inference pods with inference-specialized chips.</li>
        <li>Batching as a primary optimization knob.</li>
        <li>Model size vs efficiency tradeoff (70B for prestige, 13B for profit); host multiple right-sized models.</li>
        <li>Proper quantization to reduce memory movement and watts (FP16 to INT8 can quickly double TPS/W).</li>
        <li>Speculative decoding with a small quantized draft model verified by a larger model in batch.</li>
        <li>Energy-aware routing to the node with lowest token energy cost.</li>
        <li>Split inference with tiered architecture.</li>
        <li>Maximize GPU utilization (target 75-85%).</li>
        <li>Context-length optimization to avoid long idle sessions.</li>
        <li>Directly power AI nodes from DC plants to reduce conversion losses.</li>
      </ol>
    </section>

    <section class="panel">
      <h2>Optimization Knobs Ranking</h2>
      <ol>
        <li>GPU Utilization %</li>
        <li>Batching</li>
        <li>Model Size vs Efficiency Tradeoff</li>
        <li>Quantization</li>
        <li>Energy Cost / Cooling Efficiency</li>
        <li>Revenue per GPU-hour</li>
      </ol>
    </section>
  </main>

  <script src="site.js"></script>
</body>
</html>
